{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importamos librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import librosa\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from sklearn.calibration import LabelEncoder\n",
    "from keras.layers import LSTM, Dense, Dropout, Conv1D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparativos antes de la RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_audio(file_path,max_len):\n",
    "    # Cargar el audio\n",
    "    signal, sr = librosa.load(file_path,sr=96000)\n",
    "    # Realizar preénfasis\n",
    "    #filter_audio = librosa.effects.preemphasis(signal)\n",
    "    # Extraer MFCCs\n",
    "    mfcc = librosa.feature.mfcc(y=signal, sr=sr, n_mfcc=13)\n",
    "\n",
    "    # Realizar padding o recorte\n",
    "    if mfcc.shape[1] < max_len:\n",
    "        num_zeros = max_len - mfcc.shape[1]\n",
    "        padded_mfcc = np.pad(mfcc, ((0, 0), (0, num_zeros)), mode='constant', constant_values=0)\n",
    "        return padded_mfcc\n",
    "    else:\n",
    "        mfcc = mfcc[:, :max_len]\n",
    "        return mfcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = 'Data'\n",
    "LABELS = ['happy','cat', 'bed']\n",
    "\n",
    "mfccs = []\n",
    "labels = []\n",
    "\n",
    "padding = 500\n",
    "\n",
    "for label in LABELS:\n",
    "    path_file = DATA_DIR + f'/{label}'\n",
    "    for file in os.listdir(path_file):\n",
    "        file_path = path_file + f'/{file}'\n",
    "        \n",
    "        mfcc = preprocess_audio(file_path,padding)\n",
    "        \n",
    "        mfccs.append(mfcc)\n",
    "        labels.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convertimos las listas a numpy array\n",
    "mfccs_array = np.array(mfccs) #Matriz 3d (5187, 13, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Codificación realizada:\n",
      "bed -> 0\n",
      "cat -> 1\n",
      "happy -> 2\n"
     ]
    }
   ],
   "source": [
    "# Convertir etiquetas a números\n",
    "le = LabelEncoder()\n",
    "labels_encoded = le.fit_transform(labels)\n",
    "\n",
    "print(\"Codificación realizada:\")\n",
    "for idx, label in enumerate(le.classes_):\n",
    "    print(f\"{label} -> {idx}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividir datos en entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(mfccs_array, labels_encoded, test_size=0.2, random_state=312)\n",
    "\n",
    "# Crear modelo LSTM (Long short-term memory)\n",
    "model = Sequential([\n",
    "    Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(X_train.shape[1], X_train.shape[2])),\n",
    "    LSTM(128, return_sequences=True),\n",
    "    Dropout(0.3),\n",
    "    LSTM(128),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(3, activation='softmax')  # 3 clases: 'happy', 'cat', 'bed'\n",
    "])\n",
    "\n",
    "# Compilar el modelo\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "130/130 [==============================] - 4s 16ms/step - loss: 0.8493 - accuracy: 0.6030 - val_loss: 0.6172 - val_accuracy: 0.7678\n",
      "Epoch 2/15\n",
      "130/130 [==============================] - 2s 12ms/step - loss: 0.6706 - accuracy: 0.7137 - val_loss: 0.5543 - val_accuracy: 0.7688\n",
      "Epoch 3/15\n",
      "130/130 [==============================] - 2s 13ms/step - loss: 0.5625 - accuracy: 0.7689 - val_loss: 0.5723 - val_accuracy: 0.7861\n",
      "Epoch 4/15\n",
      "130/130 [==============================] - 2s 12ms/step - loss: 0.5349 - accuracy: 0.7884 - val_loss: 0.4782 - val_accuracy: 0.8054\n",
      "Epoch 5/15\n",
      "130/130 [==============================] - 2s 12ms/step - loss: 0.4827 - accuracy: 0.8089 - val_loss: 0.4546 - val_accuracy: 0.8304\n",
      "Epoch 6/15\n",
      "130/130 [==============================] - 2s 12ms/step - loss: 0.4506 - accuracy: 0.8195 - val_loss: 0.5277 - val_accuracy: 0.8131\n",
      "Epoch 7/15\n",
      "130/130 [==============================] - 2s 13ms/step - loss: 0.4258 - accuracy: 0.8313 - val_loss: 0.4654 - val_accuracy: 0.8170\n",
      "Epoch 8/15\n",
      "130/130 [==============================] - 2s 12ms/step - loss: 0.4071 - accuracy: 0.8385 - val_loss: 0.4294 - val_accuracy: 0.8439\n",
      "Epoch 9/15\n",
      "130/130 [==============================] - 2s 12ms/step - loss: 0.3894 - accuracy: 0.8527 - val_loss: 0.3954 - val_accuracy: 0.8478\n",
      "Epoch 10/15\n",
      "130/130 [==============================] - 2s 12ms/step - loss: 0.3970 - accuracy: 0.8443 - val_loss: 0.3820 - val_accuracy: 0.8622\n",
      "Epoch 11/15\n",
      "130/130 [==============================] - 2s 13ms/step - loss: 0.3491 - accuracy: 0.8648 - val_loss: 0.3896 - val_accuracy: 0.8507\n",
      "Epoch 12/15\n",
      "130/130 [==============================] - 2s 12ms/step - loss: 0.3415 - accuracy: 0.8648 - val_loss: 0.3848 - val_accuracy: 0.8507\n",
      "Epoch 13/15\n",
      "130/130 [==============================] - 2s 13ms/step - loss: 0.3156 - accuracy: 0.8814 - val_loss: 0.3654 - val_accuracy: 0.8574\n",
      "Epoch 14/15\n",
      "130/130 [==============================] - 2s 12ms/step - loss: 0.3133 - accuracy: 0.8836 - val_loss: 0.3462 - val_accuracy: 0.8834\n",
      "Epoch 15/15\n",
      "130/130 [==============================] - 2s 12ms/step - loss: 0.2872 - accuracy: 0.8911 - val_loss: 0.3183 - val_accuracy: 0.8863\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1bb58903fd0>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Entrenar el modelo\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=15, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33/33 [==============================] - 0s 4ms/step - loss: 0.3183 - accuracy: 0.8863\n",
      "Loss: 0.318325012922287\n",
      "Accuracy: 0.8863198161125183\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Loss: {loss}\")\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "audio_bed.wav\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "bed : 61.15589737892151%\n",
      "cat : 32.22641050815582%\n",
      "happy : 6.617692857980728%\n",
      "\n",
      "La palabra predicha es: bed\n",
      "\n",
      "audio_cat.wav\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "bed : 19.175419211387634%\n",
      "cat : 78.44647765159607%\n",
      "happy : 2.3781079798936844%\n",
      "\n",
      "La palabra predicha es: cat\n",
      "\n",
      "audio_happy.wav\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "bed : 1.6778495162725449%\n",
      "cat : 96.5961754322052%\n",
      "happy : 1.725967787206173%\n",
      "\n",
      "La palabra predicha es: cat\n",
      "\n"
     ]
    }
   ],
   "source": [
    "file_paths = ['audio_bed.wav','audio_cat.wav','audio_happy.wav']\n",
    "\n",
    "for file_path in file_paths:\n",
    "    print(file_path)\n",
    "    preprocessed_audio = preprocess_audio(file_path,padding)\n",
    "    preprocessed_audio = preprocessed_audio.reshape(1, preprocessed_audio.shape[0], preprocessed_audio.shape[1])  # Convertir a formato (1, features, tiempo)\n",
    "    prediction = model.predict(preprocessed_audio)\n",
    "\n",
    "    print(f\"{le.inverse_transform([0])[0]} : {prediction[0][0] * 100}%\")\n",
    "    print(f\"{le.inverse_transform([1])[0]} : {prediction[0][1] * 100}%\")\n",
    "    print(f\"{le.inverse_transform([2])[0]} : {prediction[0][2] * 100}%\")\n",
    "\n",
    "    predicted_label_encoded = np.argmax(prediction, axis=1)[0]\n",
    "    predicted_label = le.inverse_transform([predicted_label_encoded])[0]\n",
    "    print(f\"\\nLa palabra predicha es: {predicted_label}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Guardamos el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenar el modelo\n",
    "model.save('GUI/modelo.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
