{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importamos librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import librosa\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from sklearn.calibration import LabelEncoder\n",
    "from keras.layers import LSTM, Dense, Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparativos antes de la RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_audio(file_path,max_len):\n",
    "    # Cargar el audio\n",
    "    signal, sr = librosa.load(file_path,sr=96000)\n",
    "    # Realizar preénfasis\n",
    "    #filter_audio = librosa.effects.preemphasis(signal)\n",
    "    # Extraer MFCCs\n",
    "    mfcc = librosa.feature.mfcc(y=signal, sr=sr, n_mfcc=13)\n",
    "\n",
    "    # Realizar padding o recorte\n",
    "    if mfcc.shape[1] < max_len:\n",
    "        num_zeros = max_len - mfcc.shape[1]\n",
    "        padded_mfcc = np.pad(mfcc, ((0, 0), (0, num_zeros)), mode='constant', constant_values=0)\n",
    "        return padded_mfcc\n",
    "    else:\n",
    "        mfcc = mfcc[:, :max_len]\n",
    "        return mfcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = 'Data'\n",
    "LABELS = ['happy','cat', 'bed']\n",
    "\n",
    "mfccs = []\n",
    "labels = []\n",
    "\n",
    "padding = 500\n",
    "\n",
    "for label in LABELS:\n",
    "    path_file = DATA_DIR + f'/{label}'\n",
    "    for file in os.listdir(path_file):\n",
    "        file_path = path_file + f'/{file}'\n",
    "        \n",
    "        mfcc = preprocess_audio(file_path,padding)\n",
    "        \n",
    "        mfccs.append(mfcc)\n",
    "        labels.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convertimos las listas a numpy array\n",
    "mfccs_array = np.array(mfccs) #Matriz 3d (5187, 13, max_len + 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Codificación realizada:\n",
      "bed -> 0\n",
      "cat -> 1\n",
      "happy -> 2\n"
     ]
    }
   ],
   "source": [
    "# Convertir etiquetas a números\n",
    "le = LabelEncoder()\n",
    "labels_encoded = le.fit_transform(labels)\n",
    "\n",
    "print(\"Codificación realizada:\")\n",
    "for idx, label in enumerate(le.classes_):\n",
    "    print(f\"{label} -> {idx}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividir datos en entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(mfccs_array, labels_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "# Crear modelo LSTM (Long short-term memory)\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(X_train.shape[1], X_train.shape[2]), return_sequences=True))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(LSTM(128, return_sequences=True))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(LSTM(128))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(3, activation='softmax'))  # 3 clases: 'happy', 'cat', 'bed'\n",
    "\n",
    "# Compilar el modelo\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "130/130 [==============================] - 17s 93ms/step - loss: 0.9306 - accuracy: 0.5495 - val_loss: 0.7514 - val_accuracy: 0.6802\n",
      "Epoch 2/15\n",
      "130/130 [==============================] - 9s 70ms/step - loss: 0.7010 - accuracy: 0.6992 - val_loss: 0.6556 - val_accuracy: 0.7206\n",
      "Epoch 3/15\n",
      "130/130 [==============================] - 9s 73ms/step - loss: 0.5743 - accuracy: 0.7628 - val_loss: 0.5963 - val_accuracy: 0.7611\n",
      "Epoch 4/15\n",
      "130/130 [==============================] - 11s 82ms/step - loss: 0.5204 - accuracy: 0.7896 - val_loss: 0.5422 - val_accuracy: 0.7890\n",
      "Epoch 5/15\n",
      "130/130 [==============================] - 11s 84ms/step - loss: 0.4752 - accuracy: 0.8079 - val_loss: 0.5072 - val_accuracy: 0.7996\n",
      "Epoch 6/15\n",
      "130/130 [==============================] - 10s 78ms/step - loss: 0.4510 - accuracy: 0.8260 - val_loss: 0.4387 - val_accuracy: 0.8372\n",
      "Epoch 7/15\n",
      "130/130 [==============================] - 11s 88ms/step - loss: 0.4024 - accuracy: 0.8424 - val_loss: 0.4468 - val_accuracy: 0.8304\n",
      "Epoch 8/15\n",
      "130/130 [==============================] - 11s 86ms/step - loss: 0.3754 - accuracy: 0.8588 - val_loss: 0.4287 - val_accuracy: 0.8526\n",
      "Epoch 9/15\n",
      "130/130 [==============================] - 11s 85ms/step - loss: 0.3396 - accuracy: 0.8720 - val_loss: 0.4144 - val_accuracy: 0.8391\n",
      "Epoch 10/15\n",
      "130/130 [==============================] - 10s 78ms/step - loss: 0.3232 - accuracy: 0.8797 - val_loss: 0.4344 - val_accuracy: 0.8478\n",
      "Epoch 11/15\n",
      "130/130 [==============================] - 11s 81ms/step - loss: 0.2903 - accuracy: 0.8891 - val_loss: 0.4803 - val_accuracy: 0.8362\n",
      "Epoch 12/15\n",
      "130/130 [==============================] - 10s 75ms/step - loss: 0.2770 - accuracy: 0.8908 - val_loss: 0.4025 - val_accuracy: 0.8622\n",
      "Epoch 13/15\n",
      "130/130 [==============================] - 10s 76ms/step - loss: 0.2555 - accuracy: 0.9029 - val_loss: 0.3614 - val_accuracy: 0.8632\n",
      "Epoch 14/15\n",
      "130/130 [==============================] - 10s 76ms/step - loss: 0.2382 - accuracy: 0.9108 - val_loss: 0.4101 - val_accuracy: 0.8574\n",
      "Epoch 15/15\n",
      "130/130 [==============================] - 10s 76ms/step - loss: 0.2201 - accuracy: 0.9115 - val_loss: 0.3886 - val_accuracy: 0.8516\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1b379fdf8d0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Entrenar el modelo\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=15, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33/33 [==============================] - 1s 36ms/step - loss: 0.3886 - accuracy: 0.8516\n",
      "Loss: 0.38864654302597046\n",
      "Accuracy: 0.8516377806663513\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Loss: {loss}\")\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 32ms/step\n",
      "bed : 22.82307893037796%\n",
      "cat : 2.5985581800341606%\n",
      "happy : 74.57835674285889%\n",
      "\n",
      "La palabra predicha es: happy\n"
     ]
    }
   ],
   "source": [
    "file_path = 'audio_happy.wav'\n",
    "preprocessed_audio = preprocess_audio(file_path,padding)\n",
    "preprocessed_audio = preprocessed_audio.reshape(1, preprocessed_audio.shape[0], preprocessed_audio.shape[1])  # Convertir a formato (1, features, tiempo)\n",
    "prediction = model.predict(preprocessed_audio)\n",
    "\n",
    "print(f\"{le.inverse_transform([0])[0]} : {prediction[0][0] * 100}%\")\n",
    "print(f\"{le.inverse_transform([1])[0]} : {prediction[0][1] * 100}%\")\n",
    "print(f\"{le.inverse_transform([2])[0]} : {prediction[0][2] * 100}%\")\n",
    "\n",
    "predicted_label_encoded = np.argmax(prediction, axis=1)[0]\n",
    "predicted_label = le.inverse_transform([predicted_label_encoded])[0]\n",
    "print(f\"\\nLa palabra predicha es: {predicted_label}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
