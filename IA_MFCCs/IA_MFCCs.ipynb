{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importamos librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import librosa\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from sklearn.calibration import LabelEncoder\n",
    "from keras.layers import LSTM, Dense, Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparativos antes de la RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_audio(file_path,max_len):\n",
    "    # Cargar el audio\n",
    "    signal, sr = librosa.load(file_path,sr=96000)\n",
    "    # Realizar preénfasis\n",
    "    #filter_audio = librosa.effects.preemphasis(signal)\n",
    "    # Extraer MFCCs\n",
    "    mfcc = librosa.feature.mfcc(y=signal, sr=sr, n_mfcc=13)\n",
    "\n",
    "    # Realizar padding o recorte\n",
    "    if mfcc.shape[1] < max_len:\n",
    "        num_zeros = max_len - mfcc.shape[1]\n",
    "        padded_mfcc = np.pad(mfcc, ((0, 0), (0, num_zeros)), mode='constant', constant_values=0)\n",
    "        return padded_mfcc\n",
    "    else:\n",
    "        mfcc = mfcc[:, :max_len]\n",
    "        return mfcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = 'Data'\n",
    "LABELS = ['happy','cat', 'bed']\n",
    "\n",
    "mfccs = []\n",
    "labels = []\n",
    "\n",
    "padding = 500\n",
    "\n",
    "for label in LABELS:\n",
    "    path_file = DATA_DIR + f'/{label}'\n",
    "    for file in os.listdir(path_file):\n",
    "        file_path = path_file + f'/{file}'\n",
    "        \n",
    "        mfcc = preprocess_audio(file_path,padding)\n",
    "        \n",
    "        mfccs.append(mfcc)\n",
    "        labels.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convertimos las listas a numpy array\n",
    "mfccs_array = np.array(mfccs) #Matriz 3d (5187, 13, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Codificación realizada:\n",
      "bed -> 0\n",
      "cat -> 1\n",
      "happy -> 2\n"
     ]
    }
   ],
   "source": [
    "# Convertir etiquetas a números\n",
    "le = LabelEncoder()\n",
    "labels_encoded = le.fit_transform(labels)\n",
    "\n",
    "print(\"Codificación realizada:\")\n",
    "for idx, label in enumerate(le.classes_):\n",
    "    print(f\"{label} -> {idx}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividir datos en entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(mfccs_array, labels_encoded, test_size=0.2, random_state=312)\n",
    "\n",
    "# Crear modelo LSTM (Long short-term memory)\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(X_train.shape[1], X_train.shape[2]), return_sequences=True))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(LSTM(128, return_sequences=True))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(LSTM(128))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(3, activation='softmax'))  # 3 clases: 'happy', 'cat', 'bed'\n",
    "\n",
    "# Compilar el modelo\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "130/130 [==============================] - 24s 144ms/step - loss: 0.9511 - accuracy: 0.5343 - val_loss: 0.8244 - val_accuracy: 0.6301\n",
      "Epoch 2/10\n",
      "130/130 [==============================] - 17s 134ms/step - loss: 0.6976 - accuracy: 0.7014 - val_loss: 0.6308 - val_accuracy: 0.7370\n",
      "Epoch 3/10\n",
      "130/130 [==============================] - 18s 141ms/step - loss: 0.5843 - accuracy: 0.7479 - val_loss: 0.5283 - val_accuracy: 0.7755\n",
      "Epoch 4/10\n",
      "130/130 [==============================] - 18s 136ms/step - loss: 0.5042 - accuracy: 0.7922 - val_loss: 0.4909 - val_accuracy: 0.7977\n",
      "Epoch 5/10\n",
      "130/130 [==============================] - 17s 133ms/step - loss: 0.5073 - accuracy: 0.7978 - val_loss: 0.4589 - val_accuracy: 0.8179\n",
      "Epoch 6/10\n",
      "130/130 [==============================] - 19s 148ms/step - loss: 0.4361 - accuracy: 0.8253 - val_loss: 0.4817 - val_accuracy: 0.8141\n",
      "Epoch 7/10\n",
      "130/130 [==============================] - 18s 139ms/step - loss: 0.4023 - accuracy: 0.8412 - val_loss: 0.4490 - val_accuracy: 0.8237\n",
      "Epoch 8/10\n",
      "130/130 [==============================] - 18s 139ms/step - loss: 0.3777 - accuracy: 0.8537 - val_loss: 0.4182 - val_accuracy: 0.8372\n",
      "Epoch 9/10\n",
      "130/130 [==============================] - 19s 146ms/step - loss: 0.3413 - accuracy: 0.8725 - val_loss: 0.3880 - val_accuracy: 0.8555\n",
      "Epoch 10/10\n",
      "130/130 [==============================] - 19s 145ms/step - loss: 0.3131 - accuracy: 0.8788 - val_loss: 0.3925 - val_accuracy: 0.8536\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1dee8209610>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Entrenar el modelo\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33/33 [==============================] - 2s 62ms/step - loss: 0.3925 - accuracy: 0.8536\n",
      "Loss: 0.39247018098831177\n",
      "Accuracy: 0.8535645604133606\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Loss: {loss}\")\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "audio_bed.wav\n",
      "1/1 [==============================] - 1s 940ms/step\n",
      "bed : 40.73328077793121%\n",
      "cat : 14.031212031841278%\n",
      "happy : 45.23550570011139%\n",
      "\n",
      "La palabra predicha es: happy\n",
      "\n",
      "audio_cat.wav\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "bed : 24.552831053733826%\n",
      "cat : 60.27589440345764%\n",
      "happy : 15.171276032924652%\n",
      "\n",
      "La palabra predicha es: cat\n",
      "\n",
      "audio_happy.wav\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "bed : 24.098268151283264%\n",
      "cat : 62.86776065826416%\n",
      "happy : 13.033977150917053%\n",
      "\n",
      "La palabra predicha es: cat\n",
      "\n"
     ]
    }
   ],
   "source": [
    "file_paths = ['audio_bed.wav','audio_cat.wav','audio_happy.wav']\n",
    "\n",
    "for file_path in file_paths:\n",
    "    print(file_path)\n",
    "    preprocessed_audio = preprocess_audio(file_path,padding)\n",
    "    preprocessed_audio = preprocessed_audio.reshape(1, preprocessed_audio.shape[0], preprocessed_audio.shape[1])  # Convertir a formato (1, features, tiempo)\n",
    "    prediction = model.predict(preprocessed_audio)\n",
    "\n",
    "    print(f\"{le.inverse_transform([0])[0]} : {prediction[0][0] * 100}%\")\n",
    "    print(f\"{le.inverse_transform([1])[0]} : {prediction[0][1] * 100}%\")\n",
    "    print(f\"{le.inverse_transform([2])[0]} : {prediction[0][2] * 100}%\")\n",
    "\n",
    "    predicted_label_encoded = np.argmax(prediction, axis=1)[0]\n",
    "    predicted_label = le.inverse_transform([predicted_label_encoded])[0]\n",
    "    print(f\"\\nLa palabra predicha es: {predicted_label}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Guardamos el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenar el modelo\n",
    "model.save('GUI/modelo.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
